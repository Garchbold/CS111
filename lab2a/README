George ArchboldProject 2aCS111README	I began the file with #define _GNU_SOURCE so that I could use the GNU defined functions for the test and set and the compare and swap. Alternatively I could have written my own versions, but this was more convenient. Then I included stdio, stdlib, and string for basic library function usage. I included time so that I could implement the timer. I included optarg so that I could parse the input options. And lastly I included pthread so that I could use the thread implementation. 

	I made my two plots using R. In graph 1, you can clearly see that the time per operation decreases as the number of iterations increases for one thread. In graph 2, the plots from top to bottom are the spin lock, the compare and swap, the mutex, and then unprotected. the compare and swap, spin lock, and unprotected are all very close in time, while the spin lock linearly increases in time per operation as the number of threads increases. I tested my code, by running each of the different types of protection, with a different number of threads and iterations. By using my knowledge of the efficiency of each type of protection, I was able to detect when the code was not working properly. QUESTION 2A.1A:Why does it take this many threads or iterations to result in failure?	With less threads and iterations, there are less total operations. And if there are less total operations then there is a smaller probability that the critical code runs over itself and causes a failure. Conversely, the more operations executed by the program, the less guarantee you have for atomicity. QUESTION 2A.1B:Why does a significantly smaller number of iterations so seldom fail?	      Smaller number of iterations seldom fail for similar reasons as above. Smaller number of iterations assures safer code with less chance to interfere with atomicity. This is the trend I noticed as I increased number of iterations for no yield:      10 threads x 10 iterations x (add + subtract) = 200 operationsper operation: 4608 nanoseconds10 threads x 100 iterations x (add + subtract) = 2000 operationsper operation: 146 nanoseconds10 threads x 500 iterations x (add + subtract) = 10000 operationsper operation: 36 nanoseconds10 threads x 1000 iterations x (add + subtract) = 20000 operationsper operation: 19 nanosecondsQUESTION 2A.2A:Why does the average cost per operation drop with increasing iterations?		The average cost per operation drops because we know that when running this program, the real costly part is the thread creation and the overhead from the thread function call. So the more work you do after the threads are created the lower the time per operation will be because each additional iteration is much cheaper than the thread creation.		QUESTION 2A.2B:How do we know what the “correct” cost is?	The “correct” cost is the time of program execution without all the additional overhead from thread creation, thread joining and context switchingQUESTION 2A.2C:Why are the --yield runs so much slower?  Where is the extra time going?	The –yield runs are so much slower, because each thread is constantly giving up the CPU rather than running for the appropriate time allotted. The extra time is wasted by the constant and unnecessary context switching of each thread every time it yields the CPU.QUESTION 2A.2D:Can we get valid timings if we are using --yield?  How, or why not?	In order to get a valid timing for the –yield option, we would have to exclude the time for all the context switches in our total time count. We would have to make sure that our timer was only running for code execution, and not waiting for all the context switches to occur.QUESTION 2A.3A:Why do all of the options perform similarly for low numbers of threads?		When there are a less number of threads running concurrently, there is less inter-thread competition, and this greatly reduced the time that each thread has to wait on the previous thread to finish. As you increase the number of threads, you also increase the amount that they compete for CPU time and delay thread completion.QUESTION 2A.3B:Why do the three protected operations slow down as the number of threads rises?	The three protected operations slow down as the number of threads rises because they are all dependent on one lock. As you introduce more threads, that increases the number of threads that are trying to get access to that lock so that they can execute their critical code, however the effecting of waiting for lock access is a bottleneck.QUESTION 2A.3C:Why are spin-locks so expensive for large numbers of threads?		Rather than each thread just running through, and then allowing the next thread to call, the spinning thread wastes valuable CPU time, and prevents any other threads from running. An easy solution is putting the thread to sleep, and allowing another thread to run. This accomplishes the same thing, and does not eat up the CPU time like spinning does.